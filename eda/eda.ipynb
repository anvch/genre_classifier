{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff6463f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deep_translator in c:\\users\\22che\\miniconda3\\lib\\site-packages (1.11.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from deep_translator) (4.13.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from deep_translator) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2024.12.14)\n",
      "Requirement already satisfied: nltk in c:\\users\\22che\\miniconda3\\lib\\site-packages (3.9.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: click in c:\\users\\22che\\miniconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\22che\\miniconda3\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\22che\\miniconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\22che\\miniconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\22che\\miniconda3\\lib\\site-packages (from sentence_transformers) (4.66.4)\n",
      "Collecting torch>=1.11.0 (from sentence_transformers)\n",
      "  Downloading torch-2.9.1-cp310-cp310-win_amd64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\22che\\miniconda3\\lib\\site-packages (from sentence_transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\22che\\miniconda3\\lib\\site-packages (from sentence_transformers) (1.15.2)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence_transformers)\n",
      "  Downloading huggingface_hub-1.1.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\22che\\miniconda3\\lib\\site-packages (from sentence_transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\22che\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.9.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Collecting shellingham (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence_transformers)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\22che\\miniconda3\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence_transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.25.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\22che\\miniconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.12.14)\n",
      "Requirement already satisfied: anyio in c:\\users\\22che\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence_transformers) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\22che\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence_transformers) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence_transformers) (0.14.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence_transformers) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence_transformers) (1.2.2)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from typer-slim->huggingface-hub>=0.20.0->sentence_transformers) (8.1.7)\n",
      "Downloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Downloading torch-2.9.1-cp310-cp310-win_amd64.whl (111.0 MB)\n",
      "   ---------------------------------------- 0.0/111.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 10.5/111.0 MB 54.4 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 21.8/111.0 MB 52.8 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 32.0/111.0 MB 53.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 42.5/111.0 MB 51.9 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 51.1/111.0 MB 49.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 62.7/111.0 MB 50.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 73.1/111.0 MB 50.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 83.9/111.0 MB 50.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 96.2/111.0 MB 51.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 106.2/111.0 MB 51.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.9/111.0 MB 51.7 MB/s eta 0:00:01\n",
      "   --------------------------------------- 111.0/111.0 MB 47.2 MB/s eta 0:00:00\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   -------------------------------------- - 11.5/12.0 MB 55.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 50.1 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 566.1/566.1 kB ? eta 0:00:00\n",
      "Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 5.0/6.3 MB 23.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 21.5 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 51.3 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB ? eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, safetensors, torch, huggingface-hub, tokenizers, transformers, sentence_transformers\n",
      "Successfully installed huggingface-hub-0.36.0 mpmath-1.3.0 safetensors-0.6.2 sentence_transformers-5.1.2 sympy-1.14.0 tokenizers-0.22.1 torch-2.9.1 transformers-4.57.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from textblob) (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\22che\\miniconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\22che\\miniconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\22che\\miniconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\22che\\miniconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\22che\\miniconda3\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "   ---------------------------------------- 0.0/624.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 624.3/624.3 kB 7.7 MB/s eta 0:00:00\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.19.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\22che\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%pip install deep_translator\n",
    "%pip install nltk\n",
    "%pip install sentence_transformers\n",
    "%pip install textblob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from textblob import TextBlob\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import time\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6204bbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5134856, 11)\n",
      "               title  tag     artist  year   views  \\\n",
      "0          Killa Cam  rap    Cam'ron  2004  173166   \n",
      "1         Can I Live  rap      JAY-Z  1996  468624   \n",
      "2  Forgive Me Father  rap   Fabolous  2003    4743   \n",
      "3       Down and Out  rap    Cam'ron  2004  144404   \n",
      "4             Fly In  rap  Lil Wayne  2005   78271   \n",
      "\n",
      "                                       features  \\\n",
      "0                   {\"Cam\\\\'ron\",\"Opera Steve\"}   \n",
      "1                                            {}   \n",
      "2                                            {}   \n",
      "3  {\"Cam\\\\'ron\",\"Kanye West\",\"Syleena Johnson\"}   \n",
      "4                                            {}   \n",
      "\n",
      "                                              lyrics  id language_cld3  \\\n",
      "0  [Chorus: Opera Steve & Cam'ron]\\nKilla Cam, Ki...   1            en   \n",
      "1  [Produced by Irv Gotti]\\n\\n[Intro]\\nYeah, hah,...   3            en   \n",
      "2  Maybe cause I'm eatin\\nAnd these bastards fien...   4            en   \n",
      "3  [Produced by Kanye West and Brian Miller]\\n\\n[...   5            en   \n",
      "4  [Intro]\\nSo they ask me\\n\"Young boy\\nWhat you ...   6            en   \n",
      "\n",
      "  language_ft language  \n",
      "0          en       en  \n",
      "1          en       en  \n",
      "2          en       en  \n",
      "3          en       en  \n",
      "4          en       en  \n"
     ]
    }
   ],
   "source": [
    "lyrics = pd.read_parquet(\"../data/song_lyrics.parquet\")\n",
    "print(lyrics.shape)\n",
    "print(lyrics.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8b2c524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   language    count\n",
      "0        en  3374198\n",
      "1        es   275432\n",
      "2      None   226918\n",
      "3        fr   189436\n",
      "4        pt   167947\n",
      "..      ...      ...\n",
      "80       mt        5\n",
      "81       uz        4\n",
      "82       tg        3\n",
      "83       bs        1\n",
      "84       gu        1\n",
      "\n",
      "[85 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Metrics for each language counts\n",
    "\n",
    "language_counts = duckdb.query(\"\"\"\n",
    "    SELECT language, COUNT(*) AS count\n",
    "    FROM lyrics\n",
    "    GROUP BY language\n",
    "    ORDER BY count DESC\n",
    "\"\"\").to_df()\n",
    "print(language_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c724e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  language  count\n",
      "0       ja  42637\n",
      "1       ko  27979\n",
      "2       zh   8813\n",
      "3       hi   1609\n",
      "4       ta    646\n",
      "5       bn    579\n",
      "6       pa    131\n",
      "7       te     92\n",
      "8       ml     49\n"
     ]
    }
   ],
   "source": [
    "# We only care about indian dialects, chinese, japanese, and korean\n",
    "\n",
    "lang_codes = ['hi', 'pa', 'ta', 'te', 'ml', 'bn', 'zh','ja', 'ko']\n",
    "\n",
    "input_path = \"../data/song_lyrics.parquet\"\n",
    "output_path = \"../data/song_lyrics_asian.parquet\"\n",
    "\n",
    "duckdb.query(f\"\"\"\n",
    "    COPY (\n",
    "        SELECT *,\n",
    "             CASE\n",
    "                WHEN language IN ('hi', 'pa', 'ta', 'te', 'ml', 'bn', 'gu', 'mr') THEN 'Indian'\n",
    "                WHEN language IN ('zh') THEN 'Chinese'\n",
    "                WHEN language = 'ja' THEN 'Japanese'\n",
    "                WHEN language = 'ko' THEN 'Korean'\n",
    "                ELSE 'Other'\n",
    "             END AS region_group\n",
    "        FROM '{input_path}'\n",
    "        WHERE language IN {lang_codes}\n",
    "    )\n",
    "    TO '{output_path}' (FORMAT PARQUET);\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "asian_language_counts = duckdb.query(f\"\"\"\n",
    "    SELECT language, COUNT(*) AS count\n",
    "    FROM '{output_path}'\n",
    "    GROUP BY language\n",
    "    ORDER BY count DESC\n",
    "\"\"\").to_df()\n",
    "print(asian_language_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9093f613",
   "metadata": {},
   "outputs": [],
   "source": [
    "asian_lyrics = pd.read_parquet(\"../data/song_lyrics_asian.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84817a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DataFrame shape: (81344, 21)\n",
      "First 5 rows of the joined DataFrame:\n",
      "              title   tag           artist  year   views  \\\n",
      "0    Lovesick Girls   pop        BLACKPINK  2020  845243   \n",
      "1           FiNALLY  rock             BiSH  2019      11   \n",
      "2          NO SWEET  rock             BiSH  2019       4   \n",
      "3  Its OK To Be Sad   pop   (Janice Vidal)  2021    6064   \n",
      "4        Next Level   pop         aespa ()  2021   92350   \n",
      "\n",
      "                features                                             lyrics  \\\n",
      "0                     {}  [블랙핑크 \"Lovesick Girls\" 가사]\\n\\n[Intro: All]\\nLo...   \n",
      "1                     {}  [BiSH「FiNALLY」歌詞]\\n\\n動く日常には\\n目と目を合わせることばっか\\n遠い...   \n",
      "2                     {}  [BiSH「NO SWEET」歌詞]\\n\\n始まりは突然\\n遠ざかる幽霊たち\\nなけなしのお...   \n",
      "3  {\"衛蘭 (Janice Vidal)\"}  [衞蘭「It's OK To Be Sad」歌詞]\\n\\n[主歌一]\\n你想見他 想見他\\n...   \n",
      "4       {\"​aespa (에스파)\"}  [에스파 \"Next Level\" 가사]\\n\\n[Verse 1: Karina, Gis...   \n",
      "\n",
      "        id language_cld3 language_ft  ... energy danceability key  loudness  \\\n",
      "0  5909351            ko          ko  ...  0.732        0.658   6    -4.762   \n",
      "1  6414594            ja          ja  ...  0.980        0.462   9    -1.708   \n",
      "2  6414614            ja          ja  ...  0.501        0.576   3    -8.851   \n",
      "3  6777793            zh          zh  ...  0.535        0.534   4    -6.925   \n",
      "4  6779785            ko          ko  ...  0.852        0.820  11    -2.567   \n",
      "\n",
      "   mode  speechiness  instrumentalness  liveness  valence    tempo  \n",
      "0     1       0.0654          0.000000    0.1300    0.409  127.995  \n",
      "1     0       0.3190          0.885000    0.0678    0.145  133.031  \n",
      "2     1       0.0264          0.000001    0.1690    0.219  112.957  \n",
      "3     1       0.1560          0.000000    0.1050    0.427  156.199  \n",
      "4     0       0.1660          0.000006    0.0907    0.820  109.036  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "Joined DataFrame shape: (462, 27)\n",
      "Joined df size: 462\n"
     ]
    }
   ],
   "source": [
    "conn = duckdb.connect(database=':memory:', read_only=False)\n",
    "#Using asian songs data to extract top 100 songs from each region based on views\n",
    "conn.register('lyrics_df', asian_lyrics)\n",
    "\n",
    "#spotify kaggle dataset and loading it into a parquet file\n",
    "file_path = '../data/song_metadata.parquet'\n",
    "song_metadata = pd.read_parquet(file_path)\n",
    "\n",
    "# remove duplicates, ranking on popularity\n",
    "song_metadata = (\n",
    "    song_metadata.sort_values('popularity', ascending=False)\n",
    "             .drop_duplicates(subset=['track_name', 'artists'], keep='first')\n",
    ")\n",
    "print(f\"Training DataFrame shape: {song_metadata.shape}\")\n",
    "\n",
    "# normalize text - remove punctuation, lowercase, remove unnecessary spaces\n",
    "def normalize_text(s):\n",
    "    if pd.isnull(s):\n",
    "        return ''\n",
    "    # Lowercase and trim spaces\n",
    "    s = s.lower().strip()\n",
    "    # Normalize Unicode to standard forms (e.g., full-width → half-width)\n",
    "    s = unicodedata.normalize('NFKC', s)\n",
    "    # Keep:\n",
    "    # - English letters and digits (\\w)\n",
    "    # - Spaces (\\s)\n",
    "    # - Chinese characters (\\u4e00-\\u9fff, \\u3400-\\u4dbf)\n",
    "    # - Japanese Hiragana + Katakana (\\u3040-\\u30ff)\n",
    "    # - Hindi / Devanagari (\\u0900-\\u097F)\n",
    "    allowed = r'[^\\w\\s\\u4e00-\\u9fff\\u3400-\\u4dbf\\u3040-\\u30ff\\u0900-\\u097F]'\n",
    "    s = re.sub(allowed, '', s)\n",
    "    # Collapse multiple spaces\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "# create a rough join key for title/artist\n",
    "asian_lyrics['join_key'] = (asian_lyrics['title'].fillna('') + ' ' + asian_lyrics['artist'].fillna('')).apply(normalize_text)\n",
    "song_metadata['join_key'] = (song_metadata['track_name'].fillna('') + ' ' + song_metadata['artists'].fillna('')).apply(normalize_text)\n",
    "\n",
    "conn = duckdb.connect(database=':memory:', read_only=False)\n",
    "conn.register('lyrics_data', asian_lyrics)\n",
    "conn.register('train_data', song_metadata)\n",
    "\n",
    "# joining them both together and now this is our final csv\n",
    "joined_df = conn.execute(\"\"\"\n",
    "    SELECT\n",
    "        t1.*,\n",
    "        t2.popularity,\n",
    "        t2.duration_ms,\n",
    "        t2.acousticness,\n",
    "        t2.explicit,\n",
    "        t2.energy,\n",
    "        t2.danceability,\n",
    "        t2.key,\n",
    "        t2.loudness,\n",
    "        t2.mode,\n",
    "        t2.speechiness,\n",
    "        t2.instrumentalness,\n",
    "        t2.liveness,\n",
    "        t2.valence,\n",
    "        t2.tempo\n",
    "    FROM lyrics_data AS t1\n",
    "    LEFT JOIN train_data AS t2\n",
    "    ON t1.join_key = t2.join_key\n",
    "    WHERE t2.popularity is NOT NULL\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"First 5 rows of the joined DataFrame:\")\n",
    "print(joined_df.head())\n",
    "print(f\"Joined DataFrame shape: {joined_df.shape}\")\n",
    "output_file_path = '../data/asian_lyrics_w_metadata.parquet'\n",
    "joined_df.to_parquet(output_file_path, engine='fastparquet', index=False)\n",
    "print(\"Joined df size:\", len(joined_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08dbd2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to clean and translate lyrics\n",
    "\n",
    "def remove_square_brackets(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove anything inside [...] including nested cases\n",
    "        return re.sub(r'\\[.*?\\]', '', text).strip()\n",
    "    return text\n",
    "\n",
    "joined_df['clean_lyrics'] = joined_df['lyrics'].apply(remove_square_brackets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92a0ef71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\22che\\AppData\\Local\\Temp\\ipykernel_13148\\941859159.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sample_df = joined_df.groupby('language', group_keys=False).apply(lambda x: x.sample(min(len(x), N)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in sample_df: 186\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m sample_df \u001b[38;5;241m=\u001b[39m joined_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x), N)))\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of rows in sample_df: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sample_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m sample_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlyrics_translated\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msample_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclean_lyrics\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranslate_to_english\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\22che\\miniconda3\\lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\22che\\miniconda3\\lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\22che\\miniconda3\\lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\22che\\miniconda3\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\22che\\miniconda3\\lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[21], line 6\u001b[0m, in \u001b[0;36mtranslate_to_english\u001b[1;34m(text, retries, delay)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(retries):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 6\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m GoogleTranslator(source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtranslate(text)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def translate_to_english(text, retries=3, delay=0.5):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            time.sleep(delay)\n",
    "            return GoogleTranslator(source='auto', target='en').translate(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt+1} failed: {e}\")\n",
    "            time.sleep(delay * (attempt + 1))  # exponential backoff\n",
    "    return text  # fallback if all retries fail\n",
    "\n",
    "N = 50  # choose however many you want\n",
    "sample_df = joined_df.groupby('language', group_keys=False).apply(lambda x: x.sample(min(len(x), N)))\n",
    "print(f\"Number of rows in sample_df: {len(sample_df)}\")\n",
    "sample_df['lyrics_translated'] = sample_df['clean_lyrics'].apply(translate_to_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2810da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_parquet(\"../data/asian_songs_translated_w_metadata.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "053e0c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, extract features from the translated lyrics\n",
    "translated_lyrics = pd.read_parquet(\"../data/asian_songs_translated_w_metadata.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30a05f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\22che\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\22che\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\22che\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def extract_text_features(text):\n",
    "    tokens = preprocess_text(text)\n",
    "    if len(tokens) == 0:\n",
    "        return {\n",
    "            'word_count': 0,\n",
    "            'unique_words': 0,\n",
    "            'repetition_ratio': 0.0,\n",
    "            'lexical_diversity': 0.0\n",
    "        }\n",
    "    unique = set(tokens)\n",
    "    word_count = len(tokens)\n",
    "    unique_words = len(unique)\n",
    "    repetition_ratio = 1 - (unique_words / word_count)\n",
    "    lexical_diversity = unique_words / word_count\n",
    "    return {\n",
    "        'word_count': word_count,\n",
    "        'unique_words': unique_words,\n",
    "        'repetition_ratio': repetition_ratio,\n",
    "        'lexical_diversity': lexical_diversity\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9fc3ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = translated_lyrics['lyrics_translated'].apply(extract_text_features).apply(pd.Series)\n",
    "translated_lyrics[['word_count', 'unique_words', 'repetition_ratio', 'lexical_diversity']] = text_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db6514e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 6/6 [00:06<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "# semantic features\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # small, fast model\n",
    "embeddings = model.encode(translated_lyrics['lyrics_translated'].tolist(), show_progress_bar=True)\n",
    "\n",
    "\n",
    "translated_lyrics['sentiment_polarity'] = translated_lyrics['lyrics_translated'].apply(\n",
    "    lambda x: TextBlob(x).sentiment.polarity if isinstance(x, str) else 0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fbe4d7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_lyrics.to_parquet(\"../data/asian_songs_translated_w_metadata_lyric_features.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
